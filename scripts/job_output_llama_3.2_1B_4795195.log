/mnt/parscratch/users/aca22yn/anaconda/envs/prune_llm/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/mnt/parscratch/users/aca22yn/anaconda/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
torch 2.5.1
transformers 4.46.3
accelerate 1.1.1
# of gpus:  1
Script started successfully.
loading llm model meta-llama/Llama-2-7b-chat-hf
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [00:46<00:46, 46.85s/it]Downloading shards: 100%|██████████| 2/2 [01:02<00:00, 28.77s/it]Downloading shards: 100%|██████████| 2/2 [01:02<00:00, 31.48s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Model loaded successfully.
Tokenizer loaded successfully.
use device  cuda:0
Starting pruning with method: sparsegpt
Starting ...
Ready.
0 self_attn.q_proj
Pruning ...
0 self_attn.k_proj
Pruning ...
0 self_attn.v_proj
Pruning ...
0 self_attn.o_proj
Pruning ...
0 mlp.gate_proj
Pruning ...
0 mlp.up_proj
Pruning ...
0 mlp.down_proj
Pruning ...
1 self_attn.q_proj
Pruning ...
1 self_attn.k_proj
Pruning ...
1 self_attn.v_proj
Pruning ...
1 self_attn.o_proj
Pruning ...
1 mlp.gate_proj
Pruning ...
1 mlp.up_proj
Pruning ...
1 mlp.down_proj
Pruning ...
2 self_attn.q_proj
Pruning ...
2 self_attn.k_proj
Pruning ...
2 self_attn.v_proj
Pruning ...
2 self_attn.o_proj
Pruning ...
2 mlp.gate_proj
Pruning ...
2 mlp.up_proj
Pruning ...
2 mlp.down_proj
Pruning ...
3 self_attn.q_proj
Pruning ...
3 self_attn.k_proj
Pruning ...
3 self_attn.v_proj
Pruning ...
3 self_attn.o_proj
Pruning ...
3 mlp.gate_proj
Pruning ...
3 mlp.up_proj
Pruning ...
3 mlp.down_proj
Pruning ...
4 self_attn.q_proj
Pruning ...
4 self_attn.k_proj
Pruning ...
4 self_attn.v_proj
Pruning ...
4 self_attn.o_proj
Pruning ...
4 mlp.gate_proj
Pruning ...
4 mlp.up_proj
Pruning ...
4 mlp.down_proj
Pruning ...
5 self_attn.q_proj
Pruning ...
5 self_attn.k_proj
Pruning ...
5 self_attn.v_proj
Pruning ...
5 self_attn.o_proj
Pruning ...
5 mlp.gate_proj
Pruning ...
5 mlp.up_proj
Pruning ...
5 mlp.down_proj
Pruning ...
6 self_attn.q_proj
Pruning ...
6 self_attn.k_proj
Pruning ...
6 self_attn.v_proj
Pruning ...
6 self_attn.o_proj
Pruning ...
6 mlp.gate_proj
Pruning ...
6 mlp.up_proj
Pruning ...
6 mlp.down_proj
Pruning ...
7 self_attn.q_proj
Pruning ...
7 self_attn.k_proj
Pruning ...
7 self_attn.v_proj
Pruning ...
7 self_attn.o_proj
Pruning ...
7 mlp.gate_proj
Pruning ...
7 mlp.up_proj
Pruning ...
7 mlp.down_proj
Pruning ...
8 self_attn.q_proj
Pruning ...
8 self_attn.k_proj
Pruning ...
8 self_attn.v_proj
Pruning ...
8 self_attn.o_proj
Pruning ...
8 mlp.gate_proj
Pruning ...
8 mlp.up_proj
Pruning ...
8 mlp.down_proj
Pruning ...
9 self_attn.q_proj
Pruning ...
9 self_attn.k_proj
Pruning ...
9 self_attn.v_proj
Pruning ...
9 self_attn.o_proj
Pruning ...
9 mlp.gate_proj
Pruning ...
9 mlp.up_proj
Pruning ...
9 mlp.down_proj
Pruning ...
10 self_attn.q_proj
Pruning ...
10 self_attn.k_proj
Pruning ...
10 self_attn.v_proj
Pruning ...
10 self_attn.o_proj
Pruning ...
10 mlp.gate_proj
Pruning ...
10 mlp.up_proj
Pruning ...
10 mlp.down_proj
Pruning ...
11 self_attn.q_proj
Pruning ...
11 self_attn.k_proj
Pruning ...
11 self_attn.v_proj
Pruning ...
11 self_attn.o_proj
Pruning ...
11 mlp.gate_proj
Pruning ...
11 mlp.up_proj
Pruning ...
11 mlp.down_proj
Pruning ...
12 self_attn.q_proj
Pruning ...
12 self_attn.k_proj
Pruning ...
12 self_attn.v_proj
Pruning ...
12 self_attn.o_proj
Pruning ...
12 mlp.gate_proj
Pruning ...
12 mlp.up_proj
Pruning ...
12 mlp.down_proj
Pruning ...
13 self_attn.q_proj
Pruning ...
13 self_attn.k_proj
Pruning ...
13 self_attn.v_proj
Pruning ...
13 self_attn.o_proj
Pruning ...
13 mlp.gate_proj
Pruning ...
13 mlp.up_proj
Pruning ...
13 mlp.down_proj
Pruning ...
14 self_attn.q_proj
Pruning ...
14 self_attn.k_proj
Pruning ...
14 self_attn.v_proj
Pruning ...
14 self_attn.o_proj
Pruning ...
14 mlp.gate_proj
Pruning ...
14 mlp.up_proj
Pruning ...
14 mlp.down_proj
Pruning ...
15 self_attn.q_proj
Pruning ...
15 self_attn.k_proj
Pruning ...
15 self_attn.v_proj
Pruning ...
15 self_attn.o_proj
Pruning ...
15 mlp.gate_proj
Pruning ...
15 mlp.up_proj
Pruning ...
15 mlp.down_proj
Pruning ...
16 self_attn.q_proj
Pruning ...
16 self_attn.k_proj
Pruning ...
16 self_attn.v_proj
Pruning ...
16 self_attn.o_proj
Pruning ...
16 mlp.gate_proj
Pruning ...
16 mlp.up_proj
Pruning ...
16 mlp.down_proj
Pruning ...
17 self_attn.q_proj
Pruning ...
17 self_attn.k_proj
Pruning ...
17 self_attn.v_proj
Pruning ...
17 self_attn.o_proj
Pruning ...
17 mlp.gate_proj
Pruning ...
17 mlp.up_proj
Pruning ...
17 mlp.down_proj
Pruning ...
18 self_attn.q_proj
Pruning ...
18 self_attn.k_proj
Pruning ...
18 self_attn.v_proj
Pruning ...
18 self_attn.o_proj
Pruning ...
18 mlp.gate_proj
Pruning ...
18 mlp.up_proj
Pruning ...
18 mlp.down_proj
Pruning ...
19 self_attn.q_proj
Pruning ...
19 self_attn.k_proj
Pruning ...
19 self_attn.v_proj
Pruning ...
19 self_attn.o_proj
Pruning ...
19 mlp.gate_proj
Pruning ...
19 mlp.up_proj
Pruning ...
19 mlp.down_proj
Pruning ...
20 self_attn.q_proj
Pruning ...
20 self_attn.k_proj
Pruning ...
20 self_attn.v_proj
Pruning ...
20 self_attn.o_proj
Pruning ...
20 mlp.gate_proj
Pruning ...
20 mlp.up_proj
Pruning ...
20 mlp.down_proj
Pruning ...
21 self_attn.q_proj
Pruning ...
21 self_attn.k_proj
Pruning ...
21 self_attn.v_proj
Pruning ...
21 self_attn.o_proj
Pruning ...
21 mlp.gate_proj
Pruning ...
21 mlp.up_proj
Pruning ...
21 mlp.down_proj
Pruning ...
22 self_attn.q_proj
Pruning ...
22 self_attn.k_proj
Pruning ...
22 self_attn.v_proj
Pruning ...
22 self_attn.o_proj
Pruning ...
22 mlp.gate_proj
Pruning ...
22 mlp.up_proj
Pruning ...
22 mlp.down_proj
Pruning ...
23 self_attn.q_proj
Pruning ...
23 self_attn.k_proj
Pruning ...
23 self_attn.v_proj
Pruning ...
23 self_attn.o_proj
Pruning ...
23 mlp.gate_proj
Pruning ...
23 mlp.up_proj
Pruning ...
23 mlp.down_proj
Pruning ...
24 self_attn.q_proj
Pruning ...
24 self_attn.k_proj
Pruning ...
24 self_attn.v_proj
Pruning ...
24 self_attn.o_proj
Pruning ...
24 mlp.gate_proj
Pruning ...
24 mlp.up_proj
Pruning ...
24 mlp.down_proj
Pruning ...
25 self_attn.q_proj
Pruning ...
25 self_attn.k_proj
Pruning ...
25 self_attn.v_proj
Pruning ...
25 self_attn.o_proj
Pruning ...
25 mlp.gate_proj
Pruning ...
25 mlp.up_proj
Pruning ...
25 mlp.down_proj
Pruning ...
26 self_attn.q_proj
Pruning ...
26 self_attn.k_proj
Pruning ...
26 self_attn.v_proj
Pruning ...
26 self_attn.o_proj
Pruning ...
26 mlp.gate_proj
Pruning ...
26 mlp.up_proj
Pruning ...
26 mlp.down_proj
Pruning ...
27 self_attn.q_proj
Pruning ...
27 self_attn.k_proj
Pruning ...
27 self_attn.v_proj
Pruning ...
27 self_attn.o_proj
Pruning ...
27 mlp.gate_proj
Pruning ...
27 mlp.up_proj
Pruning ...
27 mlp.down_proj
Pruning ...
28 self_attn.q_proj
Pruning ...
28 self_attn.k_proj
Pruning ...
28 self_attn.v_proj
Pruning ...
28 self_attn.o_proj
Pruning ...
28 mlp.gate_proj
Pruning ...
28 mlp.up_proj
Pruning ...
28 mlp.down_proj
Pruning ...
29 self_attn.q_proj
Pruning ...
29 self_attn.k_proj
Pruning ...
29 self_attn.v_proj
Pruning ...
29 self_attn.o_proj
Pruning ...
29 mlp.gate_proj
Pruning ...
29 mlp.up_proj
Pruning ...
29 mlp.down_proj
Pruning ...
30 self_attn.q_proj
Pruning ...
30 self_attn.k_proj
Pruning ...
30 self_attn.v_proj
Pruning ...
30 self_attn.o_proj
Pruning ...
30 mlp.gate_proj
Pruning ...
30 mlp.up_proj
Pruning ...
30 mlp.down_proj
Pruning ...
31 self_attn.q_proj
Pruning ...
31 self_attn.k_proj
Pruning ...
31 self_attn.v_proj
Pruning ...
31 self_attn.o_proj
Pruning ...
31 mlp.gate_proj
Pruning ...
31 mlp.up_proj
Pruning ...
31 mlp.down_proj
Pruning ...
Pruning completed.
Estimating SNR after pruning...
Layer: model.embed_tokens.weight | MSE: 0.000060 | SNR: 6.767974
Computing pruning error...
Layer: model.embed_tokens.weight | Pruning Error: 0.000000
Layer: model.layers.0.self_attn.q_proj.weight | Pruning Error: 190.750000
Layer: model.layers.0.self_attn.k_proj.weight | Pruning Error: 376.000000
Layer: model.layers.0.self_attn.v_proj.weight | Pruning Error: 382.000000
Layer: model.layers.0.self_attn.o_proj.weight | Pruning Error: 438.000000
Layer: model.layers.0.mlp.gate_proj.weight | Pruning Error: 3652.000000
Layer: model.layers.0.mlp.up_proj.weight | Pruning Error: 3572.000000
Layer: model.layers.0.mlp.down_proj.weight | Pruning Error: 4236.000000
Layer: model.layers.0.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.0.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.1.self_attn.q_proj.weight | Pruning Error: 1600.000000
Layer: model.layers.1.self_attn.k_proj.weight | Pruning Error: 1727.000000
Layer: model.layers.1.self_attn.v_proj.weight | Pruning Error: 391.000000
Layer: model.layers.1.self_attn.o_proj.weight | Pruning Error: 448.750000
Layer: model.layers.1.mlp.gate_proj.weight | Pruning Error: 4656.000000
Layer: model.layers.1.mlp.up_proj.weight | Pruning Error: 4108.000000
Layer: model.layers.1.mlp.down_proj.weight | Pruning Error: 2922.000000
Layer: model.layers.1.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.1.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.2.self_attn.q_proj.weight | Pruning Error: 2332.000000
Layer: model.layers.2.self_attn.k_proj.weight | Pruning Error: 2286.000000
Layer: model.layers.2.self_attn.v_proj.weight | Pruning Error: 961.500000
Layer: model.layers.2.self_attn.o_proj.weight | Pruning Error: 978.000000
Layer: model.layers.2.mlp.gate_proj.weight | Pruning Error: 4532.000000
Layer: model.layers.2.mlp.up_proj.weight | Pruning Error: 3884.000000
Layer: model.layers.2.mlp.down_proj.weight | Pruning Error: 3660.000000
Layer: model.layers.2.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.2.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.3.self_attn.q_proj.weight | Pruning Error: 2214.000000
Layer: model.layers.3.self_attn.k_proj.weight | Pruning Error: 2256.000000
Layer: model.layers.3.self_attn.v_proj.weight | Pruning Error: 808.500000
Layer: model.layers.3.self_attn.o_proj.weight | Pruning Error: 839.000000
Layer: model.layers.3.mlp.gate_proj.weight | Pruning Error: 4344.000000
Layer: model.layers.3.mlp.up_proj.weight | Pruning Error: 3716.000000
Layer: model.layers.3.mlp.down_proj.weight | Pruning Error: 3630.000000
Layer: model.layers.3.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.3.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.4.self_attn.q_proj.weight | Pruning Error: 2522.000000
Layer: model.layers.4.self_attn.k_proj.weight | Pruning Error: 2476.000000
Layer: model.layers.4.self_attn.v_proj.weight | Pruning Error: 912.500000
Layer: model.layers.4.self_attn.o_proj.weight | Pruning Error: 924.500000
Layer: model.layers.4.mlp.gate_proj.weight | Pruning Error: 4452.000000
Layer: model.layers.4.mlp.up_proj.weight | Pruning Error: 3642.000000
Layer: model.layers.4.mlp.down_proj.weight | Pruning Error: 3508.000000
Layer: model.layers.4.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.4.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.5.self_attn.q_proj.weight | Pruning Error: 2610.000000
Layer: model.layers.5.self_attn.k_proj.weight | Pruning Error: 2664.000000
Layer: model.layers.5.self_attn.v_proj.weight | Pruning Error: 929.500000
Layer: model.layers.5.self_attn.o_proj.weight | Pruning Error: 1026.000000
Layer: model.layers.5.mlp.gate_proj.weight | Pruning Error: 4484.000000
Layer: model.layers.5.mlp.up_proj.weight | Pruning Error: 3658.000000
Layer: model.layers.5.mlp.down_proj.weight | Pruning Error: 3586.000000
Layer: model.layers.5.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.5.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.6.self_attn.q_proj.weight | Pruning Error: 2222.000000
Layer: model.layers.6.self_attn.k_proj.weight | Pruning Error: 2226.000000
Layer: model.layers.6.self_attn.v_proj.weight | Pruning Error: 805.500000
Layer: model.layers.6.self_attn.o_proj.weight | Pruning Error: 833.000000
Layer: model.layers.6.mlp.gate_proj.weight | Pruning Error: 4644.000000
Layer: model.layers.6.mlp.up_proj.weight | Pruning Error: 3658.000000
Layer: model.layers.6.mlp.down_proj.weight | Pruning Error: 3744.000000
Layer: model.layers.6.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.6.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.7.self_attn.q_proj.weight | Pruning Error: 2282.000000
Layer: model.layers.7.self_attn.k_proj.weight | Pruning Error: 2258.000000
Layer: model.layers.7.self_attn.v_proj.weight | Pruning Error: 844.000000
Layer: model.layers.7.self_attn.o_proj.weight | Pruning Error: 881.000000
Layer: model.layers.7.mlp.gate_proj.weight | Pruning Error: 4720.000000
Layer: model.layers.7.mlp.up_proj.weight | Pruning Error: 3774.000000
Layer: model.layers.7.mlp.down_proj.weight | Pruning Error: 3754.000000
Layer: model.layers.7.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.7.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.8.self_attn.q_proj.weight | Pruning Error: 2388.000000
Layer: model.layers.8.self_attn.k_proj.weight | Pruning Error: 2372.000000
Layer: model.layers.8.self_attn.v_proj.weight | Pruning Error: 918.000000
Layer: model.layers.8.self_attn.o_proj.weight | Pruning Error: 948.500000
Layer: model.layers.8.mlp.gate_proj.weight | Pruning Error: 4580.000000
Layer: model.layers.8.mlp.up_proj.weight | Pruning Error: 3884.000000
Layer: model.layers.8.mlp.down_proj.weight | Pruning Error: 3798.000000
Layer: model.layers.8.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.8.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.9.self_attn.q_proj.weight | Pruning Error: 2528.000000
Layer: model.layers.9.self_attn.k_proj.weight | Pruning Error: 2548.000000
Layer: model.layers.9.self_attn.v_proj.weight | Pruning Error: 942.500000
Layer: model.layers.9.self_attn.o_proj.weight | Pruning Error: 979.000000
Layer: model.layers.9.mlp.gate_proj.weight | Pruning Error: 4432.000000
Layer: model.layers.9.mlp.up_proj.weight | Pruning Error: 3904.000000
Layer: model.layers.9.mlp.down_proj.weight | Pruning Error: 3772.000000
Layer: model.layers.9.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.9.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.10.self_attn.q_proj.weight | Pruning Error: 2426.000000
Layer: model.layers.10.self_attn.k_proj.weight | Pruning Error: 2436.000000
Layer: model.layers.10.self_attn.v_proj.weight | Pruning Error: 925.500000
Layer: model.layers.10.self_attn.o_proj.weight | Pruning Error: 971.500000
Layer: model.layers.10.mlp.gate_proj.weight | Pruning Error: 4316.000000
Layer: model.layers.10.mlp.up_proj.weight | Pruning Error: 3958.000000
Layer: model.layers.10.mlp.down_proj.weight | Pruning Error: 3826.000000
Layer: model.layers.10.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.10.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.11.self_attn.q_proj.weight | Pruning Error: 2002.000000
Layer: model.layers.11.self_attn.k_proj.weight | Pruning Error: 1965.000000
Layer: model.layers.11.self_attn.v_proj.weight | Pruning Error: 988.000000
Layer: model.layers.11.self_attn.o_proj.weight | Pruning Error: 1076.000000
Layer: model.layers.11.mlp.gate_proj.weight | Pruning Error: 4280.000000
Layer: model.layers.11.mlp.up_proj.weight | Pruning Error: 3990.000000
Layer: model.layers.11.mlp.down_proj.weight | Pruning Error: 3818.000000
Layer: model.layers.11.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.11.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.12.self_attn.q_proj.weight | Pruning Error: 2260.000000
Layer: model.layers.12.self_attn.k_proj.weight | Pruning Error: 2304.000000
Layer: model.layers.12.self_attn.v_proj.weight | Pruning Error: 959.500000
Layer: model.layers.12.self_attn.o_proj.weight | Pruning Error: 1024.000000
Layer: model.layers.12.mlp.gate_proj.weight | Pruning Error: 4232.000000
Layer: model.layers.12.mlp.up_proj.weight | Pruning Error: 4050.000000
Layer: model.layers.12.mlp.down_proj.weight | Pruning Error: 3906.000000
Layer: model.layers.12.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.12.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.13.self_attn.q_proj.weight | Pruning Error: 2214.000000
Layer: model.layers.13.self_attn.k_proj.weight | Pruning Error: 2230.000000
Layer: model.layers.13.self_attn.v_proj.weight | Pruning Error: 1025.000000
Layer: model.layers.13.self_attn.o_proj.weight | Pruning Error: 1111.000000
Layer: model.layers.13.mlp.gate_proj.weight | Pruning Error: 4192.000000
Layer: model.layers.13.mlp.up_proj.weight | Pruning Error: 4120.000000
Layer: model.layers.13.mlp.down_proj.weight | Pruning Error: 3976.000000
Layer: model.layers.13.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.13.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.14.self_attn.q_proj.weight | Pruning Error: 2200.000000
Layer: model.layers.14.self_attn.k_proj.weight | Pruning Error: 2212.000000
Layer: model.layers.14.self_attn.v_proj.weight | Pruning Error: 996.000000
Layer: model.layers.14.self_attn.o_proj.weight | Pruning Error: 1079.000000
Layer: model.layers.14.mlp.gate_proj.weight | Pruning Error: 4180.000000
Layer: model.layers.14.mlp.up_proj.weight | Pruning Error: 4124.000000
Layer: model.layers.14.mlp.down_proj.weight | Pruning Error: 3974.000000
Layer: model.layers.14.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.14.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.15.self_attn.q_proj.weight | Pruning Error: 2200.000000
Layer: model.layers.15.self_attn.k_proj.weight | Pruning Error: 2262.000000
Layer: model.layers.15.self_attn.v_proj.weight | Pruning Error: 1080.000000
Layer: model.layers.15.self_attn.o_proj.weight | Pruning Error: 1135.000000
Layer: model.layers.15.mlp.gate_proj.weight | Pruning Error: 4208.000000
Layer: model.layers.15.mlp.up_proj.weight | Pruning Error: 4160.000000
Layer: model.layers.15.mlp.down_proj.weight | Pruning Error: 4108.000000
Layer: model.layers.15.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.15.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.16.self_attn.q_proj.weight | Pruning Error: 2124.000000
Layer: model.layers.16.self_attn.k_proj.weight | Pruning Error: 2154.000000
Layer: model.layers.16.self_attn.v_proj.weight | Pruning Error: 1174.000000
Layer: model.layers.16.self_attn.o_proj.weight | Pruning Error: 1264.000000
Layer: model.layers.16.mlp.gate_proj.weight | Pruning Error: 4340.000000
Layer: model.layers.16.mlp.up_proj.weight | Pruning Error: 4208.000000
Layer: model.layers.16.mlp.down_proj.weight | Pruning Error: 4104.000000
Layer: model.layers.16.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.16.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.17.self_attn.q_proj.weight | Pruning Error: 2172.000000
Layer: model.layers.17.self_attn.k_proj.weight | Pruning Error: 2202.000000
Layer: model.layers.17.self_attn.v_proj.weight | Pruning Error: 1184.000000
Layer: model.layers.17.self_attn.o_proj.weight | Pruning Error: 1272.000000
Layer: model.layers.17.mlp.gate_proj.weight | Pruning Error: 4428.000000
Layer: model.layers.17.mlp.up_proj.weight | Pruning Error: 4172.000000
Layer: model.layers.17.mlp.down_proj.weight | Pruning Error: 4042.000000
Layer: model.layers.17.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.17.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.18.self_attn.q_proj.weight | Pruning Error: 2110.000000
Layer: model.layers.18.self_attn.k_proj.weight | Pruning Error: 2138.000000
Layer: model.layers.18.self_attn.v_proj.weight | Pruning Error: 1291.000000
Layer: model.layers.18.self_attn.o_proj.weight | Pruning Error: 1424.000000
Layer: model.layers.18.mlp.gate_proj.weight | Pruning Error: 4540.000000
Layer: model.layers.18.mlp.up_proj.weight | Pruning Error: 4172.000000
Layer: model.layers.18.mlp.down_proj.weight | Pruning Error: 4014.000000
Layer: model.layers.18.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.18.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.19.self_attn.q_proj.weight | Pruning Error: 2028.000000
Layer: model.layers.19.self_attn.k_proj.weight | Pruning Error: 2040.000000
Layer: model.layers.19.self_attn.v_proj.weight | Pruning Error: 1324.000000
Layer: model.layers.19.self_attn.o_proj.weight | Pruning Error: 1520.000000
Layer: model.layers.19.mlp.gate_proj.weight | Pruning Error: 4572.000000
Layer: model.layers.19.mlp.up_proj.weight | Pruning Error: 4152.000000
Layer: model.layers.19.mlp.down_proj.weight | Pruning Error: 3982.000000
Layer: model.layers.19.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.19.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.20.self_attn.q_proj.weight | Pruning Error: 2015.000000
Layer: model.layers.20.self_attn.k_proj.weight | Pruning Error: 2022.000000
Layer: model.layers.20.self_attn.v_proj.weight | Pruning Error: 1370.000000
Layer: model.layers.20.self_attn.o_proj.weight | Pruning Error: 1439.000000
Layer: model.layers.20.mlp.gate_proj.weight | Pruning Error: 4668.000000
Layer: model.layers.20.mlp.up_proj.weight | Pruning Error: 4156.000000
Layer: model.layers.20.mlp.down_proj.weight | Pruning Error: 3942.000000
Layer: model.layers.20.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.20.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.21.self_attn.q_proj.weight | Pruning Error: 1906.000000
Layer: model.layers.21.self_attn.k_proj.weight | Pruning Error: 1889.000000
Layer: model.layers.21.self_attn.v_proj.weight | Pruning Error: 1451.000000
Layer: model.layers.21.self_attn.o_proj.weight | Pruning Error: 1492.000000
Layer: model.layers.21.mlp.gate_proj.weight | Pruning Error: 4716.000000
Layer: model.layers.21.mlp.up_proj.weight | Pruning Error: 4124.000000
Layer: model.layers.21.mlp.down_proj.weight | Pruning Error: 3892.000000
Layer: model.layers.21.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.21.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.22.self_attn.q_proj.weight | Pruning Error: 2024.000000
Layer: model.layers.22.self_attn.k_proj.weight | Pruning Error: 2018.000000
Layer: model.layers.22.self_attn.v_proj.weight | Pruning Error: 1447.000000
Layer: model.layers.22.self_attn.o_proj.weight | Pruning Error: 1439.000000
Layer: model.layers.22.mlp.gate_proj.weight | Pruning Error: 4792.000000
Layer: model.layers.22.mlp.up_proj.weight | Pruning Error: 4128.000000
Layer: model.layers.22.mlp.down_proj.weight | Pruning Error: 3886.000000
Layer: model.layers.22.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.22.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.23.self_attn.q_proj.weight | Pruning Error: 2023.000000
Layer: model.layers.23.self_attn.k_proj.weight | Pruning Error: 2017.000000
Layer: model.layers.23.self_attn.v_proj.weight | Pruning Error: 1579.000000
Layer: model.layers.23.self_attn.o_proj.weight | Pruning Error: 1752.000000
Layer: model.layers.23.mlp.gate_proj.weight | Pruning Error: 4776.000000
Layer: model.layers.23.mlp.up_proj.weight | Pruning Error: 4140.000000
Layer: model.layers.23.mlp.down_proj.weight | Pruning Error: 3922.000000
Layer: model.layers.23.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.23.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.24.self_attn.q_proj.weight | Pruning Error: 1774.000000
Layer: model.layers.24.self_attn.k_proj.weight | Pruning Error: 1747.000000
Layer: model.layers.24.self_attn.v_proj.weight | Pruning Error: 1548.000000
Layer: model.layers.24.self_attn.o_proj.weight | Pruning Error: 1837.000000
Layer: model.layers.24.mlp.gate_proj.weight | Pruning Error: 4776.000000
Layer: model.layers.24.mlp.up_proj.weight | Pruning Error: 4148.000000
Layer: model.layers.24.mlp.down_proj.weight | Pruning Error: 3966.000000
Layer: model.layers.24.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.24.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.25.self_attn.q_proj.weight | Pruning Error: 1885.000000
Layer: model.layers.25.self_attn.k_proj.weight | Pruning Error: 1881.000000
Layer: model.layers.25.self_attn.v_proj.weight | Pruning Error: 1654.000000
Layer: model.layers.25.self_attn.o_proj.weight | Pruning Error: 1957.000000
Layer: model.layers.25.mlp.gate_proj.weight | Pruning Error: 4752.000000
Layer: model.layers.25.mlp.up_proj.weight | Pruning Error: 4168.000000
Layer: model.layers.25.mlp.down_proj.weight | Pruning Error: 4032.000000
Layer: model.layers.25.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.25.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.26.self_attn.q_proj.weight | Pruning Error: 1799.000000
Layer: model.layers.26.self_attn.k_proj.weight | Pruning Error: 1795.000000
Layer: model.layers.26.self_attn.v_proj.weight | Pruning Error: 1682.000000
Layer: model.layers.26.self_attn.o_proj.weight | Pruning Error: 1936.000000
Layer: model.layers.26.mlp.gate_proj.weight | Pruning Error: 4752.000000
Layer: model.layers.26.mlp.up_proj.weight | Pruning Error: 4208.000000
Layer: model.layers.26.mlp.down_proj.weight | Pruning Error: 4148.000000
Layer: model.layers.26.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.26.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.27.self_attn.q_proj.weight | Pruning Error: 2074.000000
Layer: model.layers.27.self_attn.k_proj.weight | Pruning Error: 2104.000000
Layer: model.layers.27.self_attn.v_proj.weight | Pruning Error: 1706.000000
Layer: model.layers.27.self_attn.o_proj.weight | Pruning Error: 1966.000000
Layer: model.layers.27.mlp.gate_proj.weight | Pruning Error: 4720.000000
Layer: model.layers.27.mlp.up_proj.weight | Pruning Error: 4244.000000
Layer: model.layers.27.mlp.down_proj.weight | Pruning Error: 4236.000000
Layer: model.layers.27.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.27.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.28.self_attn.q_proj.weight | Pruning Error: 1912.000000
Layer: model.layers.28.self_attn.k_proj.weight | Pruning Error: 1949.000000
Layer: model.layers.28.self_attn.v_proj.weight | Pruning Error: 1800.000000
Layer: model.layers.28.self_attn.o_proj.weight | Pruning Error: 2694.000000
Layer: model.layers.28.mlp.gate_proj.weight | Pruning Error: 4680.000000
Layer: model.layers.28.mlp.up_proj.weight | Pruning Error: 4360.000000
Layer: model.layers.28.mlp.down_proj.weight | Pruning Error: 4392.000000
Layer: model.layers.28.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.28.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.29.self_attn.q_proj.weight | Pruning Error: 1768.000000
Layer: model.layers.29.self_attn.k_proj.weight | Pruning Error: 1780.000000
Layer: model.layers.29.self_attn.v_proj.weight | Pruning Error: 1844.000000
Layer: model.layers.29.self_attn.o_proj.weight | Pruning Error: 2576.000000
Layer: model.layers.29.mlp.gate_proj.weight | Pruning Error: 4660.000000
Layer: model.layers.29.mlp.up_proj.weight | Pruning Error: 4432.000000
Layer: model.layers.29.mlp.down_proj.weight | Pruning Error: 4492.000000
Layer: model.layers.29.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.29.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.30.self_attn.q_proj.weight | Pruning Error: 1883.000000
Layer: model.layers.30.self_attn.k_proj.weight | Pruning Error: 1937.000000
Layer: model.layers.30.self_attn.v_proj.weight | Pruning Error: 1946.000000
Layer: model.layers.30.self_attn.o_proj.weight | Pruning Error: 2580.000000
Layer: model.layers.30.mlp.gate_proj.weight | Pruning Error: 4780.000000
Layer: model.layers.30.mlp.up_proj.weight | Pruning Error: 4508.000000
Layer: model.layers.30.mlp.down_proj.weight | Pruning Error: 4556.000000
Layer: model.layers.30.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.30.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.31.self_attn.q_proj.weight | Pruning Error: 1964.000000
Layer: model.layers.31.self_attn.k_proj.weight | Pruning Error: 2052.000000
Layer: model.layers.31.self_attn.v_proj.weight | Pruning Error: 1625.000000
Layer: model.layers.31.self_attn.o_proj.weight | Pruning Error: 1898.000000
Layer: model.layers.31.mlp.gate_proj.weight | Pruning Error: 5428.000000
Layer: model.layers.31.mlp.up_proj.weight | Pruning Error: 4984.000000
Layer: model.layers.31.mlp.down_proj.weight | Pruning Error: 4656.000000
Layer: model.layers.31.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.31.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.norm.weight | Pruning Error: 0.000000
Layer: lm_head.weight | Pruning Error: 0.000000
Total Pruning Error: 0.000091
******************************
layer 0 sparsity 0.700001
layer 1 sparsity 0.700001
layer 2 sparsity 0.700001
layer 3 sparsity 0.700001
layer 4 sparsity 0.700001
layer 5 sparsity 0.700001
layer 6 sparsity 0.700001
layer 7 sparsity 0.700001
layer 8 sparsity 0.700001
layer 9 sparsity 0.700001
layer 10 sparsity 0.700001
layer 11 sparsity 0.700001
layer 12 sparsity 0.700001
layer 13 sparsity 0.700001
layer 14 sparsity 0.700001
layer 15 sparsity 0.700001
layer 16 sparsity 0.700001
layer 17 sparsity 0.700001
layer 18 sparsity 0.700001
layer 19 sparsity 0.700001
layer 20 sparsity 0.700001
layer 21 sparsity 0.700001
layer 22 sparsity 0.700001
layer 23 sparsity 0.700001
layer 24 sparsity 0.700001
layer 25 sparsity 0.700001
layer 26 sparsity 0.700001
layer 27 sparsity 0.700001
layer 28 sparsity 0.700001
layer 29 sparsity 0.700001
layer 30 sparsity 0.700001
layer 31 sparsity 0.700001
sparsity sanity check 0.7000
******************************
Starting perplexity evaluation on wikitext.
evaluating on wikitext2
Loaded 2 samples in testloader.
nsamples 83
sample 0
sample 50
wikitext perplexity 24.783655166625977
