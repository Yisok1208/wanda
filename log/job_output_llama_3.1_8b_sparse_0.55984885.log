/mnt/parscratch/users/aca22yn/anaconda/envs/dissertation_llm/lib/python3.13/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
torch 2.5.1+cu118
transformers 4.50.3
accelerate 1.6.0
# of gpus:  1
loading llm model meta-llama/Llama-3.1-8B
loading llm model meta-llama/Llama-3.1-8B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:25,  8.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:06,  6.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  5.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.05s/it]
model loaded!
model.eval() called
tokenizer loaded!
use device  cuda:0
pruning starts
Starting prune_sparsegpt...
Calibration dataloader loaded.
Running model forward pass to collect calibration inputs...
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Batch fetched
Batch passed through catcher
Ready.
Traceback (most recent call last):
  File "/mnt/parscratch/users/aca22yn/wanda2/wanda/main.py", line 158, in <module>
    main()
    ~~~~^^
  File "/mnt/parscratch/users/aca22yn/wanda2/wanda/main.py", line 120, in main
    prune_sparsegpt(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parscratch/users/aca22yn/anaconda/envs/dissertation_llm/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/parscratch/users/aca22yn/wanda2/wanda/lib/prune.py", line 305, in prune_sparsegpt
    ).last_hidden_state[0]
      ^^^^^^^^^^^^^^^^^
AttributeError: 'CausalLMOutputWithPast' object has no attribute 'last_hidden_state'
