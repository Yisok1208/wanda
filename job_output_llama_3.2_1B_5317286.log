/mnt/parscratch/users/aca22yn/anaconda/envs/prune_llm/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/mnt/parscratch/users/aca22yn/anaconda/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
torch 2.5.1
transformers 4.46.3
accelerate 1.1.1
# of gpus:  1
Script started successfully.
loading llm model meta-llama/Llama-2-7b-chat-hf
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [01:34<01:34, 94.27s/it]Downloading shards: 100%|██████████| 2/2 [01:50<00:00, 48.29s/it]Downloading shards: 100%|██████████| 2/2 [01:50<00:00, 55.19s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
Model loaded successfully.
Tokenizer loaded successfully.
use device  cuda:0
Starting pruning with method: wanda
loading calibration data
dataset loading complete
  0%|          | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  3%|▎         | 1/32 [00:02<01:23,  2.71s/it]  6%|▋         | 2/32 [00:05<01:19,  2.65s/it]  9%|▉         | 3/32 [00:07<01:16,  2.63s/it] 12%|█▎        | 4/32 [00:10<01:13,  2.63s/it] 16%|█▌        | 5/32 [00:13<01:10,  2.62s/it] 19%|█▉        | 6/32 [00:15<01:08,  2.62s/it] 22%|██▏       | 7/32 [00:18<01:05,  2.62s/it] 25%|██▌       | 8/32 [00:20<01:02,  2.62s/it] 28%|██▊       | 9/32 [00:23<01:00,  2.62s/it] 31%|███▏      | 10/32 [00:26<00:57,  2.61s/it] 34%|███▍      | 11/32 [00:28<00:54,  2.61s/it] 38%|███▊      | 12/32 [00:31<00:52,  2.61s/it] 41%|████      | 13/32 [00:34<00:49,  2.61s/it] 44%|████▍     | 14/32 [00:36<00:47,  2.61s/it] 47%|████▋     | 15/32 [00:39<00:44,  2.61s/it] 50%|█████     | 16/32 [00:41<00:41,  2.61s/it] 53%|█████▎    | 17/32 [00:44<00:39,  2.61s/it] 56%|█████▋    | 18/32 [00:47<00:36,  2.61s/it] 59%|█████▉    | 19/32 [00:49<00:3pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.down_proj
pruning layer 2 name self_attn.q_proj
pruning layer 2 name self_attn.k_proj
pruning layer 2 name self_attn.v_proj
pruning layer 2 name self_attn.o_proj
pruning layer 2 name mlp.gate_proj
pruning layer 2 name mlp.up_proj
pruning layer 2 name mlp.down_proj
pruning layer 3 name self_attn.q_proj
pruning layer 3 name self_attn.k_proj
pruning layer 3 name self_attn.v_proj
pruning layer 3 name self_attn.o_proj
pruning layer 3 name mlp.gate_proj
pruning layer 3 name mlp.up_proj
pruning layer 3 name mlp.down_proj
pruning layer 4 name self_attn.q_proj
pruning layer 4 name self_attn.k_proj
pruning layer 4 name self_attn.v_proj
pruning layer 4 name self_attn.o_proj
pruning layer 4 name mlp.gate_proj
pruning layer 4 name mlp.up_proj
pruning layer 4 name mlp.down_proj
pruning layer 5 name self_attn.q_proj
pruning layer 5 name self_attn.k_proj
pruning layer 5 name self_attn.v_proj
pruning layer 5 name self_attn.o_proj
pruning layer 5 name mlp.gate_proj
pruning layer 5 name mlp.up_proj
pruning layer 5 name mlp.down_proj
pruning layer 6 name self_attn.q_proj
pruning layer 6 name self_attn.k_proj
pruning layer 6 name self_attn.v_proj
pruning layer 6 name self_attn.o_proj
pruning layer 6 name mlp.gate_proj
pruning layer 6 name mlp.up_proj
pruning layer 6 name mlp.down_proj
pruning layer 7 name self_attn.q_proj
pruning layer 7 name self_attn.k_proj
pruning layer 7 name self_attn.v_proj
pruning layer 7 name self_attn.o_proj
pruning layer 7 name mlp.gate_proj
pruning layer 7 name mlp.up_proj
pruning layer 7 name mlp.down_proj
pruning layer 8 name self_attn.q_proj
pruning layer 8 name self_attn.k_proj
pruning layer 8 name self_attn.v_proj
pruning layer 8 name self_attn.o_proj
pruning layer 8 name mlp.gate_proj
pruning layer 8 name mlp.up_proj
pruning layer 8 name mlp.down_proj
pruning layer 9 name self_attn.q_proj
pruning layer 9 name self_attn.k_proj
pruning layer 9 name self_attn.v_proj
pruning layer 9 name self_attn.o_proj
pruning layer 9 name mlp.gate_proj
pruning layer 9 name mlp.up_proj
pruning layer 9 name mlp.down_proj
pruning layer 10 name self_attn.q_proj
pruning layer 10 name self_attn.k_proj
pruning layer 10 name self_attn.v_proj
pruning layer 10 name self_attn.o_proj
pruning layer 10 name mlp.gate_proj
pruning layer 10 name mlp.up_proj
pruning layer 10 name mlp.down_proj
pruning layer 11 name self_attn.q_proj
pruning layer 11 name self_attn.k_proj
pruning layer 11 name self_attn.v_proj
pruning layer 11 name self_attn.o_proj
pruning layer 11 name mlp.gate_proj
pruning layer 11 name mlp.up_proj
pruning layer 11 name mlp.down_proj
pruning layer 12 name self_attn.q_proj
pruning layer 12 name self_attn.k_proj
pruning layer 12 name self_attn.v_proj
pruning layer 12 name self_attn.o_proj
pruning layer 12 name mlp.gate_proj
pruning layer 12 name mlp.up_proj
pruning layer 12 name mlp.down_proj
pruning layer 13 name self_attn.q_proj
pruning layer 13 name self_attn.k_proj
pruning layer 13 name self_attn.v_proj
pruning layer 13 name self_attn.o_proj
pruning layer 13 name mlp.gate_proj
pruning layer 13 name mlp.up_proj
pruning layer 13 name mlp.down_proj
pruning layer 14 name self_attn.q_proj
pruning layer 14 name self_attn.k_proj
pruning layer 14 name self_attn.v_proj
pruning layer 14 name self_attn.o_proj
pruning layer 14 name mlp.gate_proj
pruning layer 14 name mlp.up_proj
pruning layer 14 name mlp.down_proj
pruning layer 15 name self_attn.q_proj
pruning layer 15 name self_attn.k_proj
pruning layer 15 name self_attn.v_proj
pruning layer 15 name self_attn.o_proj
pruning layer 15 name mlp.gate_proj
pruning layer 15 name mlp.up_proj
pruning layer 15 name mlp.down_proj
pruning layer 16 name self_attn.q_proj
pruning layer 16 name self_attn.k_proj
pruning layer 16 name self_attn.v_proj
pruning layer 16 name self_attn.o_proj
pruning layer 16 name mlp.gate_proj
pruning layer 16 name mlp.up_proj
pruning layer 16 name mlp.down_proj
pruning layer 17 name self_attn.q_proj
pruning layer 17 name self_attn.k_proj
pruning layer 17 name self_attn.v_proj
pruning layer 17 name self_attn.o_proj
pruning layer 17 name mlp.gate_proj
pruning layer 17 name mlp.up_proj
pruning layer 17 name mlp.down_proj
pruning layer 18 name self_attn.q_proj
pruning layer 18 name self_attn.k_proj
pruning layer 18 name self_attn.v_proj
pruning layer 18 name self_attn.o_proj
pruning layer 18 name mlp.gate_proj
pruning layer 18 name mlp.up_proj
pruning layer 18 name mlp.down_proj
pruning layer 19 name self_attn.q_proj
pruning layer 19 name self_attn.k_proj
pruning layer 19 name self_attn.v_proj
pruning layer 19 name self_attn.o_proj
pruning layer 19 name mlp.gate_proj
pruning layer 19 name mlp.up_proj
pruning layer 19 name mlp.down_proj
pruning layer 20 name self_attn.q_proj
pruning layer 20 name self_attn.k_proj
pruning layer 20 name self_attn.v_proj
pruning layer 20 name self_attn.o_proj
pruning layer 20 name mlp.gate_proj
pruning layer 20 name mlp.up_proj
pruning layer 20 name mlp.down_proj
pruning layer 21 name self_attn.q_proj
pruning layer 21 name self_attn.k_proj
pruning layer 21 name self_attn.v_proj
pruning layer 21 name self_attn.o_proj
pruning layer 21 name mlp.gate_proj
pruning layer 21 name mlp.up_proj
pruning layer 21 name mlp.down_proj
pruning layer 22 name self_attn.q_proj
pruning layer 22 name self_attn.k_proj
pruning layer 22 name self_attn.v_proj
pruning layer 22 name self_attn.o_proj
pruning layer 22 name mlp.gate_proj
pruning layer 22 name mlp.up_proj
pruning layer 22 name mlp.down_proj
pruning layer 23 name self_attn.q_proj
pruning layer 23 name self_attn.k_proj
pruning layer 23 name self_attn.v_proj
pruning layer 23 name self_attn.o_proj
pruning layer 23 name mlp.gate_proj
pruning layer 23 name mlp.up_proj
pruning layer 23 name mlp.down_proj
pruning layer 24 name self_attn.q_proj
pruning layer 24 name self_attn.k_proj
pruning layer 24 name self_attn.v_proj
pruning layer 24 name self_attn.o_proj
pruning layer 24 name mlp.gate_proj
pruning layer 24 name mlp.up_proj
pruning layer 24 name mlp.down_proj
pruning layer 25 name self_attn.q_proj
pruning layer 25 name self_attn.k_proj
pruning layer 25 name self_attn.v_proj
pruning layer 25 name self_attn.o_proj
pruning layer 25 name mlp.gate_proj
pruning layer 25 name mlp.up_proj
pruning layer 25 name mlp.down_proj
pruning layer 26 name self_attn.q_proj
pruning layer 26 name self_attn.k_proj
pruning layer 26 name self_attn.v_proj
pruning layer 26 name self_attn.o_proj
pruning layer 26 name mlp.gate_proj
pruning layer 26 name mlp.up_proj
pruning layer 26 name mlp.down_proj
pruning layer 27 name self_attn.q_proj
pruning layer 27 name self_attn.k_proj
pruning layer 27 name self_attn.v_proj
pruning layer 27 name self_attn.o_proj
pruning layer 27 name mlp.gate_proj
pruning layer 27 name mlp.up_proj
pruning layer 27 name mlp.down_proj
pruning layer 28 name self_attn.q_proj
pruning layer 28 name self_attn.k_proj
pruning layer 28 name self_attn.v_proj
pruning layer 28 name self_attn.o_proj
pruning layer 28 name mlp.gate_proj
pruning layer 28 name mlp.up_proj
pruning layer 28 name mlp.down_proj
pruning layer 29 name self_attn.q_proj
pruning layer 29 name self_attn.k_proj
pruning layer 29 name self_attn.v_proj
pruning layer 29 name self_attn.o_proj
pruning layer 29 name mlp.gate_proj
pruning layer 29 name mlp.up_proj
pruning layer 29 name mlp.down_proj
pruning layer 30 name self_attn.q_proj
pruning layer 30 name self_attn.k_proj
pruning layer 30 name self_attn.v_proj
pruning layer 30 name self_attn.o_proj
pruning layer 30 name mlp.gate_proj
pruning layer 30 name mlp.up_proj
pruning layer 30 name mlp.down_proj
pruning layer 31 name self_attn.q_proj
pruning layer 31 name self_attn.k_proj
pruning layer 31 name self_attn.v_proj
3,  2.62s/it] 62%|██████▎   | 20/32 [00:52<00:31,  2.62s/it] 66%|██████▌   | 21/32 [00:54<00:28,  2.62s/it] 69%|██████▉   | 22/32 [00:57<00:26,  2.62s/it] 72%|███████▏  | 23/32 [01:00<00:23,  2.62s/it] 75%|███████▌  | 24/32 [01:02<00:20,  2.62s/it] 78%|███████▊  | 25/32 [01:05<00:18,  2.62s/it] 81%|████████▏ | 26/32 [01:08<00:15,  2.62s/it] 84%|████████▍ | 27/32 [01:10<00:13,  2.62s/it] 88%|████████▊ | 28/32 [01:13<00:10,  2.62s/it] 91%|█████████ | 29/32 [01:15<00:07,  2.62s/it] 94%|█████████▍| 30/32 [01:18<00:05,  2.62s/it] 97%|█████████▋| 31/32 [01:21<00:02,  2.62s/it]100%|██████████| 32/32 [01:23<00:00,  2.62s/it]100%|██████████| 32/32 [01:23<00:00,  2.62s/it]
pruning layer 31 name self_attn.o_proj
pruning layer 31 name mlp.gate_proj
pruning layer 31 name mlp.up_proj
pruning layer 31 name mlp.down_proj
Pruning completed.
Estimating SNR after pruning...
Layer: model.embed_tokens.weight | MSE: 0.000060 | SNR: 6.767974
Computing pruning error...
Layer: model.embed_tokens.weight | Pruning Error: 0.000000
Layer: model.layers.0.self_attn.q_proj.weight | Pruning Error: 358.750000
Layer: model.layers.0.self_attn.k_proj.weight | Pruning Error: 539.500000
Layer: model.layers.0.self_attn.v_proj.weight | Pruning Error: 583.500000
Layer: model.layers.0.self_attn.o_proj.weight | Pruning Error: 145.375000
Layer: model.layers.0.mlp.gate_proj.weight | Pruning Error: 2698.000000
Layer: model.layers.0.mlp.up_proj.weight | Pruning Error: 2608.000000
Layer: model.layers.0.mlp.down_proj.weight | Pruning Error: 3376.000000
Layer: model.layers.0.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.0.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.1.self_attn.q_proj.weight | Pruning Error: 3254.000000
Layer: model.layers.1.self_attn.k_proj.weight | Pruning Error: 3156.000000
Layer: model.layers.1.self_attn.v_proj.weight | Pruning Error: 557.500000
Layer: model.layers.1.self_attn.o_proj.weight | Pruning Error: 193.000000
Layer: model.layers.1.mlp.gate_proj.weight | Pruning Error: 3246.000000
Layer: model.layers.1.mlp.up_proj.weight | Pruning Error: 2866.000000
Layer: model.layers.1.mlp.down_proj.weight | Pruning Error: 3358.000000
Layer: model.layers.1.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.1.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.2.self_attn.q_proj.weight | Pruning Error: 2642.000000
Layer: model.layers.2.self_attn.k_proj.weight | Pruning Error: 2924.000000
Layer: model.layers.2.self_attn.v_proj.weight | Pruning Error: 776.500000
Layer: model.layers.2.self_attn.o_proj.weight | Pruning Error: 1007.500000
Layer: model.layers.2.mlp.gate_proj.weight | Pruning Error: 3408.000000
Layer: model.layers.2.mlp.up_proj.weight | Pruning Error: 2948.000000
Layer: model.layers.2.mlp.down_proj.weight | Pruning Error: 3178.000000
Layer: model.layers.2.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.2.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.3.self_attn.q_proj.weight | Pruning Error: 2346.000000
Layer: model.layers.3.self_attn.k_proj.weight | Pruning Error: 2552.000000
Layer: model.layers.3.self_attn.v_proj.weight | Pruning Error: 692.500000
Layer: model.layers.3.self_attn.o_proj.weight | Pruning Error: 900.000000
Layer: model.layers.3.mlp.gate_proj.weight | Pruning Error: 3530.000000
Layer: model.layers.3.mlp.up_proj.weight | Pruning Error: 3012.000000
Layer: model.layers.3.mlp.down_proj.weight | Pruning Error: 3334.000000
Layer: model.layers.3.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.3.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.4.self_attn.q_proj.weight | Pruning Error: 2532.000000
Layer: model.layers.4.self_attn.k_proj.weight | Pruning Error: 2638.000000
Layer: model.layers.4.self_attn.v_proj.weight | Pruning Error: 763.500000
Layer: model.layers.4.self_attn.o_proj.weight | Pruning Error: 1065.000000
Layer: model.layers.4.mlp.gate_proj.weight | Pruning Error: 3688.000000
Layer: model.layers.4.mlp.up_proj.weight | Pruning Error: 2980.000000
Layer: model.layers.4.mlp.down_proj.weight | Pruning Error: 3330.000000
Layer: model.layers.4.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.4.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.5.self_attn.q_proj.weight | Pruning Error: 2570.000000
Layer: model.layers.5.self_attn.k_proj.weight | Pruning Error: 2780.000000
Layer: model.layers.5.self_attn.v_proj.weight | Pruning Error: 804.000000
Layer: model.layers.5.self_attn.o_proj.weight | Pruning Error: 1305.000000
Layer: model.layers.5.mlp.gate_proj.weight | Pruning Error: 3794.000000
Layer: model.layers.5.mlp.up_proj.weight | Pruning Error: 3028.000000
Layer: model.layers.5.mlp.down_proj.weight | Pruning Error: 3592.000000
Layer: model.layers.5.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.5.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.6.self_attn.q_proj.weight | Pruning Error: 2342.000000
Layer: model.layers.6.self_attn.k_proj.weight | Pruning Error: 2440.000000
Layer: model.layers.6.self_attn.v_proj.weight | Pruning Error: 706.500000
Layer: model.layers.6.self_attn.o_proj.weight | Pruning Error: 1131.000000
Layer: model.layers.6.mlp.gate_proj.weight | Pruning Error: 4324.000000
Layer: model.layers.6.mlp.up_proj.weight | Pruning Error: 3314.000000
Layer: model.layers.6.mlp.down_proj.weight | Pruning Error: 4090.000000
Layer: model.layers.6.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.6.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.7.self_attn.q_proj.weight | Pruning Error: 2478.000000
Layer: model.layers.7.self_attn.k_proj.weight | Pruning Error: 2504.000000
Layer: model.layers.7.self_attn.v_proj.weight | Pruning Error: 769.500000
Layer: model.layers.7.self_attn.o_proj.weight | Pruning Error: 1229.000000
Layer: model.layers.7.mlp.gate_proj.weight | Pruning Error: 4196.000000
Layer: model.layers.7.mlp.up_proj.weight | Pruning Error: 3242.000000
Layer: model.layers.7.mlp.down_proj.weight | Pruning Error: 3842.000000
Layer: model.layers.7.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.7.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.8.self_attn.q_proj.weight | Pruning Error: 2464.000000
Layer: model.layers.8.self_attn.k_proj.weight | Pruning Error: 2492.000000
Layer: model.layers.8.self_attn.v_proj.weight | Pruning Error: 791.500000
Layer: model.layers.8.self_attn.o_proj.weight | Pruning Error: 1219.000000
Layer: model.layers.8.mlp.gate_proj.weight | Pruning Error: 3982.000000
Layer: model.layers.8.mlp.up_proj.weight | Pruning Error: 3258.000000
Layer: model.layers.8.mlp.down_proj.weight | Pruning Error: 3916.000000
Layer: model.layers.8.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.8.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.9.self_attn.q_proj.weight | Pruning Error: 2442.000000
Layer: model.layers.9.self_attn.k_proj.weight | Pruning Error: 2560.000000
Layer: model.layers.9.self_attn.v_proj.weight | Pruning Error: 812.500000
Layer: model.layers.9.self_attn.o_proj.weight | Pruning Error: 1164.000000
Layer: model.layers.9.mlp.gate_proj.weight | Pruning Error: 3850.000000
Layer: model.layers.9.mlp.up_proj.weight | Pruning Error: 3256.000000
Layer: model.layers.9.mlp.down_proj.weight | Pruning Error: 3934.000000
Layer: model.layers.9.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.9.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.10.self_attn.q_proj.weight | Pruning Error: 2410.000000
Layer: model.layers.10.self_attn.k_proj.weight | Pruning Error: 2550.000000
Layer: model.layers.10.self_attn.v_proj.weight | Pruning Error: 787.000000
Layer: model.layers.10.self_attn.o_proj.weight | Pruning Error: 1141.000000
Layer: model.layers.10.mlp.gate_proj.weight | Pruning Error: 3792.000000
Layer: model.layers.10.mlp.up_proj.weight | Pruning Error: 3298.000000
Layer: model.layers.10.mlp.down_proj.weight | Pruning Error: 4096.000000
Layer: model.layers.10.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.10.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.11.self_attn.q_proj.weight | Pruning Error: 2160.000000
Layer: model.layers.11.self_attn.k_proj.weight | Pruning Error: 2144.000000
Layer: model.layers.11.self_attn.v_proj.weight | Pruning Error: 847.500000
Layer: model.layers.11.self_attn.o_proj.weight | Pruning Error: 1314.000000
Layer: model.layers.11.mlp.gate_proj.weight | Pruning Error: 3772.000000
Layer: model.layers.11.mlp.up_proj.weight | Pruning Error: 3364.000000
Layer: model.layers.11.mlp.down_proj.weight | Pruning Error: 4068.000000
Layer: model.layers.11.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.11.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.12.self_attn.q_proj.weight | Pruning Error: 2240.000000
Layer: model.layers.12.self_attn.k_proj.weight | Pruning Error: 2376.000000
Layer: model.layers.12.self_attn.v_proj.weight | Pruning Error: 825.000000
Layer: model.layers.12.self_attn.o_proj.weight | Pruning Error: 1217.000000
Layer: model.layers.12.mlp.gate_proj.weight | Pruning Error: 3690.000000
Layer: model.layers.12.mlp.up_proj.weight | Pruning Error: 3396.000000
Layer: model.layers.12.mlp.down_proj.weight | Pruning Error: 4152.000000
Layer: model.layers.12.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.12.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.13.self_attn.q_proj.weight | Pruning Error: 2184.000000
Layer: model.layers.13.self_attn.k_proj.weight | Pruning Error: 2258.000000
Layer: model.layers.13.self_attn.v_proj.weight | Pruning Error: 881.000000
Layer: model.layers.13.self_attn.o_proj.weight | Pruning Error: 1337.000000
Layer: model.layers.13.mlp.gate_proj.weight | Pruning Error: 3668.000000
Layer: model.layers.13.mlp.up_proj.weight | Pruning Error: 3470.000000
Layer: model.layers.13.mlp.down_proj.weight | Pruning Error: 4240.000000
Layer: model.layers.13.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.13.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.14.self_attn.q_proj.weight | Pruning Error: 2174.000000
Layer: model.layers.14.self_attn.k_proj.weight | Pruning Error: 2246.000000
Layer: model.layers.14.self_attn.v_proj.weight | Pruning Error: 847.500000
Layer: model.layers.14.self_attn.o_proj.weight | Pruning Error: 1260.000000
Layer: model.layers.14.mlp.gate_proj.weight | Pruning Error: 3644.000000
Layer: model.layers.14.mlp.up_proj.weight | Pruning Error: 3466.000000
Layer: model.layers.14.mlp.down_proj.weight | Pruning Error: 4172.000000
Layer: model.layers.14.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.14.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.15.self_attn.q_proj.weight | Pruning Error: 2120.000000
Layer: model.layers.15.self_attn.k_proj.weight | Pruning Error: 2260.000000
Layer: model.layers.15.self_attn.v_proj.weight | Pruning Error: 915.000000
Layer: model.layers.15.self_attn.o_proj.weight | Pruning Error: 1427.000000
Layer: model.layers.15.mlp.gate_proj.weight | Pruning Error: 3674.000000
Layer: model.layers.15.mlp.up_proj.weight | Pruning Error: 3494.000000
Layer: model.layers.15.mlp.down_proj.weight | Pruning Error: 4420.000000
Layer: model.layers.15.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.15.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.16.self_attn.q_proj.weight | Pruning Error: 2082.000000
Layer: model.layers.16.self_attn.k_proj.weight | Pruning Error: 2184.000000
Layer: model.layers.16.self_attn.v_proj.weight | Pruning Error: 992.000000
Layer: model.layers.16.self_attn.o_proj.weight | Pruning Error: 1498.000000
Layer: model.layers.16.mlp.gate_proj.weight | Pruning Error: 3754.000000
Layer: model.layers.16.mlp.up_proj.weight | Pruning Error: 3518.000000
Layer: model.layers.16.mlp.down_proj.weight | Pruning Error: 4248.000000
Layer: model.layers.16.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.16.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.17.self_attn.q_proj.weight | Pruning Error: 2062.000000
Layer: model.layers.17.self_attn.k_proj.weight | Pruning Error: 2148.000000
Layer: model.layers.17.self_attn.v_proj.weight | Pruning Error: 993.000000
Layer: model.layers.17.self_attn.o_proj.weight | Pruning Error: 1463.000000
Layer: model.layers.17.mlp.gate_proj.weight | Pruning Error: 3780.000000
Layer: model.layers.17.mlp.up_proj.weight | Pruning Error: 3452.000000
Layer: model.layers.17.mlp.down_proj.weight | Pruning Error: 4044.000000
Layer: model.layers.17.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.17.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.18.self_attn.q_proj.weight | Pruning Error: 1958.000000
Layer: model.layers.18.self_attn.k_proj.weight | Pruning Error: 2041.000000
Layer: model.layers.18.self_attn.v_proj.weight | Pruning Error: 1082.000000
Layer: model.layers.18.self_attn.o_proj.weight | Pruning Error: 1859.000000
Layer: model.layers.18.mlp.gate_proj.weight | Pruning Error: 3824.000000
Layer: model.layers.18.mlp.up_proj.weight | Pruning Error: 3410.000000
Layer: model.layers.18.mlp.down_proj.weight | Pruning Error: 4012.000000
Layer: model.layers.18.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.18.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.19.self_attn.q_proj.weight | Pruning Error: 1903.000000
Layer: model.layers.19.self_attn.k_proj.weight | Pruning Error: 1971.000000
Layer: model.layers.19.self_attn.v_proj.weight | Pruning Error: 1099.000000
Layer: model.layers.19.self_attn.o_proj.weight | Pruning Error: 1950.000000
Layer: model.layers.19.mlp.gate_proj.weight | Pruning Error: 3844.000000
Layer: model.layers.19.mlp.up_proj.weight | Pruning Error: 3402.000000
Layer: model.layers.19.mlp.down_proj.weight | Pruning Error: 3892.000000
Layer: model.layers.19.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.19.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.20.self_attn.q_proj.weight | Pruning Error: 1923.000000
Layer: model.layers.20.self_attn.k_proj.weight | Pruning Error: 1987.000000
Layer: model.layers.20.self_attn.v_proj.weight | Pruning Error: 1130.000000
Layer: model.layers.20.self_attn.o_proj.weight | Pruning Error: 1868.000000
Layer: model.layers.20.mlp.gate_proj.weight | Pruning Error: 3886.000000
Layer: model.layers.20.mlp.up_proj.weight | Pruning Error: 3398.000000
Layer: model.layers.20.mlp.down_proj.weight | Pruning Error: 3766.000000
Layer: model.layers.20.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.20.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.21.self_attn.q_proj.weight | Pruning Error: 1837.000000
Layer: model.layers.21.self_attn.k_proj.weight | Pruning Error: 1872.000000
Layer: model.layers.21.self_attn.v_proj.weight | Pruning Error: 1202.000000
Layer: model.layers.21.self_attn.o_proj.weight | Pruning Error: 2039.000000
Layer: model.layers.21.mlp.gate_proj.weight | Pruning Error: 3928.000000
Layer: model.layers.21.mlp.up_proj.weight | Pruning Error: 3378.000000
Layer: model.layers.21.mlp.down_proj.weight | Pruning Error: 3708.000000
Layer: model.layers.21.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.21.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.22.self_attn.q_proj.weight | Pruning Error: 1913.000000
Layer: model.layers.22.self_attn.k_proj.weight | Pruning Error: 1956.000000
Layer: model.layers.22.self_attn.v_proj.weight | Pruning Error: 1210.000000
Layer: model.layers.22.self_attn.o_proj.weight | Pruning Error: 2156.000000
Layer: model.layers.22.mlp.gate_proj.weight | Pruning Error: 3972.000000
Layer: model.layers.22.mlp.up_proj.weight | Pruning Error: 3374.000000
Layer: model.layers.22.mlp.down_proj.weight | Pruning Error: 3720.000000
Layer: model.layers.22.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.22.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.23.self_attn.q_proj.weight | Pruning Error: 1883.000000
Layer: model.layers.23.self_attn.k_proj.weight | Pruning Error: 1911.000000
Layer: model.layers.23.self_attn.v_proj.weight | Pruning Error: 1324.000000
Layer: model.layers.23.self_attn.o_proj.weight | Pruning Error: 2166.000000
Layer: model.layers.23.mlp.gate_proj.weight | Pruning Error: 3972.000000
Layer: model.layers.23.mlp.up_proj.weight | Pruning Error: 3408.000000
Layer: model.layers.23.mlp.down_proj.weight | Pruning Error: 3740.000000
Layer: model.layers.23.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.23.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.24.self_attn.q_proj.weight | Pruning Error: 1784.000000
Layer: model.layers.24.self_attn.k_proj.weight | Pruning Error: 1795.000000
Layer: model.layers.24.self_attn.v_proj.weight | Pruning Error: 1300.000000
Layer: model.layers.24.self_attn.o_proj.weight | Pruning Error: 2514.000000
Layer: model.layers.24.mlp.gate_proj.weight | Pruning Error: 3992.000000
Layer: model.layers.24.mlp.up_proj.weight | Pruning Error: 3432.000000
Layer: model.layers.24.mlp.down_proj.weight | Pruning Error: 3766.000000
Layer: model.layers.24.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.24.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.25.self_attn.q_proj.weight | Pruning Error: 1813.000000
Layer: model.layers.25.self_attn.k_proj.weight | Pruning Error: 1824.000000
Layer: model.layers.25.self_attn.v_proj.weight | Pruning Error: 1401.000000
Layer: model.layers.25.self_attn.o_proj.weight | Pruning Error: 2646.000000
Layer: model.layers.25.mlp.gate_proj.weight | Pruning Error: 4012.000000
Layer: model.layers.25.mlp.up_proj.weight | Pruning Error: 3468.000000
Layer: model.layers.25.mlp.down_proj.weight | Pruning Error: 3850.000000
Layer: model.layers.25.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.25.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.26.self_attn.q_proj.weight | Pruning Error: 1759.000000
Layer: model.layers.26.self_attn.k_proj.weight | Pruning Error: 1786.000000
Layer: model.layers.26.self_attn.v_proj.weight | Pruning Error: 1450.000000
Layer: model.layers.26.self_attn.o_proj.weight | Pruning Error: 2928.000000
Layer: model.layers.26.mlp.gate_proj.weight | Pruning Error: 4054.000000
Layer: model.layers.26.mlp.up_proj.weight | Pruning Error: 3504.000000
Layer: model.layers.26.mlp.down_proj.weight | Pruning Error: 3982.000000
Layer: model.layers.26.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.26.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.27.self_attn.q_proj.weight | Pruning Error: 1895.000000
Layer: model.layers.27.self_attn.k_proj.weight | Pruning Error: 1938.000000
Layer: model.layers.27.self_attn.v_proj.weight | Pruning Error: 1458.000000
Layer: model.layers.27.self_attn.o_proj.weight | Pruning Error: 2748.000000
Layer: model.layers.27.mlp.gate_proj.weight | Pruning Error: 4074.000000
Layer: model.layers.27.mlp.up_proj.weight | Pruning Error: 3548.000000
Layer: model.layers.27.mlp.down_proj.weight | Pruning Error: 4108.000000
Layer: model.layers.27.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.27.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.28.self_attn.q_proj.weight | Pruning Error: 1791.000000
Layer: model.layers.28.self_attn.k_proj.weight | Pruning Error: 1842.000000
Layer: model.layers.28.self_attn.v_proj.weight | Pruning Error: 1552.000000
Layer: model.layers.28.self_attn.o_proj.weight | Pruning Error: 2966.000000
Layer: model.layers.28.mlp.gate_proj.weight | Pruning Error: 4048.000000
Layer: model.layers.28.mlp.up_proj.weight | Pruning Error: 3626.000000
Layer: model.layers.28.mlp.down_proj.weight | Pruning Error: 4332.000000
Layer: model.layers.28.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.28.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.29.self_attn.q_proj.weight | Pruning Error: 1712.000000
Layer: model.layers.29.self_attn.k_proj.weight | Pruning Error: 1736.000000
Layer: model.layers.29.self_attn.v_proj.weight | Pruning Error: 1569.000000
Layer: model.layers.29.self_attn.o_proj.weight | Pruning Error: 2984.000000
Layer: model.layers.29.mlp.gate_proj.weight | Pruning Error: 4082.000000
Layer: model.layers.29.mlp.up_proj.weight | Pruning Error: 3706.000000
Layer: model.layers.29.mlp.down_proj.weight | Pruning Error: 4560.000000
Layer: model.layers.29.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.29.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.30.self_attn.q_proj.weight | Pruning Error: 1719.000000
Layer: model.layers.30.self_attn.k_proj.weight | Pruning Error: 1781.000000
Layer: model.layers.30.self_attn.v_proj.weight | Pruning Error: 1646.000000
Layer: model.layers.30.self_attn.o_proj.weight | Pruning Error: 3110.000000
Layer: model.layers.30.mlp.gate_proj.weight | Pruning Error: 4324.000000
Layer: model.layers.30.mlp.up_proj.weight | Pruning Error: 3878.000000
Layer: model.layers.30.mlp.down_proj.weight | Pruning Error: 4920.000000
Layer: model.layers.30.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.30.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.31.self_attn.q_proj.weight | Pruning Error: 1776.000000
Layer: model.layers.31.self_attn.k_proj.weight | Pruning Error: 1909.000000
Layer: model.layers.31.self_attn.v_proj.weight | Pruning Error: 1369.000000
Layer: model.layers.31.self_attn.o_proj.weight | Pruning Error: 2098.000000
Layer: model.layers.31.mlp.gate_proj.weight | Pruning Error: 4696.000000
Layer: model.layers.31.mlp.up_proj.weight | Pruning Error: 4156.000000
Layer: model.layers.31.mlp.down_proj.weight | Pruning Error: 5528.000000
Layer: model.layers.31.input_layernorm.weight | Pruning Error: 0.000000
Layer: model.layers.31.post_attention_layernorm.weight | Pruning Error: 0.000000
Layer: model.norm.weight | Pruning Error: 0.000000
Layer: lm_head.weight | Pruning Error: 0.000000
Total Pruning Error: 0.000086
******************************
layer 0 sparsity 0.699950
layer 1 sparsity 0.699950
layer 2 sparsity 0.699950
layer 3 sparsity 0.699950
layer 4 sparsity 0.699950
layer 5 sparsity 0.699950
layer 6 sparsity 0.699950
layer 7 sparsity 0.699950
layer 8 sparsity 0.699950
layer 9 sparsity 0.699950
layer 10 sparsity 0.699950
layer 11 sparsity 0.699950
layer 12 sparsity 0.699950
layer 13 sparsity 0.699950
layer 14 sparsity 0.699950
layer 15 sparsity 0.699950
layer 16 sparsity 0.699950
layer 17 sparsity 0.699950
layer 18 sparsity 0.699950
layer 19 sparsity 0.699950
layer 20 sparsity 0.699950
layer 21 sparsity 0.699950
layer 22 sparsity 0.699950
layer 23 sparsity 0.699950
layer 24 sparsity 0.699950
layer 25 sparsity 0.699950
layer 26 sparsity 0.699950
layer 27 sparsity 0.699950
layer 28 sparsity 0.699950
layer 29 sparsity 0.699950
layer 30 sparsity 0.699950
layer 31 sparsity 0.699950
sparsity sanity check 0.6999
******************************
Starting perplexity evaluation on wikitext.
evaluating on wikitext2
Loaded 2 samples in testloader.
nsamples 83
sample 0
sample 50
wikitext perplexity 56.934967041015625
Model and tokenizer saved successfully.
