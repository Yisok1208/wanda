/users/aca22yn/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/users/aca22yn/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
torch 2.5.0
transformers 4.28.0
accelerate 0.18.0
# of gpus:  0
loading llm model meta-llama/Llama-3.2-1B-Instruct
use device  cuda:0
pruning starts
loading calibdation data
Downloading and preparing dataset None/en to file:///users/aca22yn/.cache/huggingface/datasets/allenai___json/en-ec45c889631c3c39/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2430.07it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 35.05it/s]
Generating train split:   0%|          | 0/364868892 [00:00<?, ? examples/s]Generating train split:   0%|          | 4435/364868892 [00:00<3:33:22, 28498.76 examples/s]Generating train split:   0%|          | 9178/364868892 [00:00<2:47:46, 36246.48 examples/s]Generating train split:   0%|          | 18137/364868892 [00:00<2:20:27, 43294.52 examples/s]Generating train split:   0%|          | 27276/364868892 [00:00<2:04:54, 48684.27 examples/s]Generating train split:   0%|          | 36862/364868892 [00:00<2:00:02, 50652.47 examples/s]Generating train split:   0%|          | 45854/364868892 [00:01<2:13:52, 45419.18 examples/s]Generating train split:   0%|          | 50510/364868892 [00:01<2:20:13, 43359.11 examples/s]Generating train split:   0%|          | 55073/364868892 [00:01<2:29:46, 40595.86 examples/s]Generating train split:   0%|          | 64140/364868892 [00:01<2:22:38, 42626.54 examples/s]Generating train split:   0%|          | 68750/364868892 [00:01<2:24:42, 42014.07 examples/s]Generating train split:   0%|          | 73387/364868892 [00:01<2:26:13, 41577.68 examples/s]Generating train split:   0%|          | 77743/364868892 [00:01<2:28:35, 40915.37 examples/s]Generating train split:   0%|          | 82339/364868892 [00:01<2:28:58, 40809.21 examples/s]Generating train split:   0%|          | 86966/364868892 [00:02<2:26:03, 41625.02 examples/s]Generating train split:   0%|          | 91398/364868892 [00:02<2:24:07, 42181.11 examples/s]Generating train split:   0%|          | 100344/364868892 [00:02<2:03:59, 49029.54 examples/s]Generating train split:   0%|          | 109336/364868892 [00:02<2:08:56, 47150.69 examples/s]Generating train split:   0%|          | 118215/364868892 [00:02<2:13:18, 45603.13 examples/s]Generating train split:   0%|          | 132051/364868892 [00:02<1:46:08, 57274.55 examples/s]Generating train split:   0%|          | 141177/364868892 [00:02<1:40:32, 60464.59 examples/s]Generating train split:   0%|          | 150440/364868892 [00:03<1:52:38, 53967.59 examples/s]Generating train split:   0%|          | 164342/364868892 [00:03<1:38:30, 61703.02 examples/s]Generating train split:   0%|          | 173282/364868892 [00:03<1:32:49, 65481.95 examples/s]Generating train split:   0%|          | 182003/364868892 [00:03<1:39:23, 61149.56 examples/s]Generating train split:   0%|          | 195882/364868892 [00:03<1:32:47, 65502.98 examples/s]Generating train split:   0%|          | 209378/364868892 [00:04<1:26:06, 70578.73 examples/s]Generating train split:   0%|          | 218473/364868892 [00:04<1:36:45, 62808.66 examples/s]Generating train split:   0%|          | 227618/364868892 [00:05<5:10:11, 19592.60 examples/s]Generating train split:   0%|          | 236617/364868892 [00:06<7:49:07, 12954.20 examples/s]Generating train split:   0%|          | 241218/364868892 [00:07<8:45:55, 11555.01 examples/s]Generating train split:   0%|          | 245816/364868892 [00:07<8:09:54, 12404.61 examples/s]Generating train split:   0%|          | 250396/364868892 [00:07<7:16:08, 13933.73 examples/s]Generating train split:   0%|          | 254993/364868892 [00:08<6:31:08, 15536.63 examples/s]Generating train split:   0%|          | 259470/364868892 [00:08<5:59:26, 16906.18 examples/s]Generating train split:   0%|          | 263922/364868892 [00:08<5:33:18, 18231.93 examples/s]Generating train split:   0%|          | 268486/364868892 [00:08<5:15:36, 19253.55 examples/s]Generating train split:   0%|          | 273202/364868892 [00:08<4:38:49, 21793.14 examples/s]Generating train split:   0%|          | 277868/364868892 [00:08<4:01:10, 25194.87 examples/s]Generating train split:   0%|          | 282396/364868892 [00:09<4:01:23, 25173.12 examples/s]Generating train split:   0%|          | 286955/364868892 [00:09<3:55:37, 25787.51 examples/s]Generating train split:   0%|          | 291456/364868892 [00:09<4:00:43, 25241.39 examples/s]Generating train split:   0%|          | 295901/364868892 [00:09<3:39:37, 27666.27 examples/s]Generating train split:   0%|          | 300440/364868892 [00:09<3:16:59, 30845.43 examples/s]Generating train split:   0%|          | 304853/364868892 [00:09<3:45:12, 26979.62 examples/s]Generating train split:   0%|          | 309452/364868892 [00:10<3:42:20, 27327.04 examples/s]Generating train split:   0%|          | 318529/364868892 [00:10<2:51:44, 35376.44 examples/s]Generating train split:   0%|          | 327690/364868892 [00:10<2:24:13, 42128.10 examples/s]Generating train split:   0%|          | 341256/364868892 [00:10<1:49:04, 55699.78 examples/s]Generating train split:   0%|          | 350356/364868892 [00:10<2:00:13, 50530.33 examples/s]Generating train split:   0%|          | 356317/364868892 [00:10<1:57:26, 51727.95 examples/s]                                                                                              Traceback (most recent call last):
  File "/mnt/parscratch/users/aca22yn/wanda/main.py", line 111, in <module>
    main()
  File "/mnt/parscratch/users/aca22yn/wanda/main.py", line 70, in main
    prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)
  File "/mnt/parscratch/users/aca22yn/wanda/lib/prune.py", line 132, in prune_wanda
    dataloader, _ = get_loaders("c4",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)
  File "/mnt/parscratch/users/aca22yn/wanda/lib/data.py", line 73, in get_loaders
    return get_c4(nsamples, seed, seqlen, tokenizer)
  File "/mnt/parscratch/users/aca22yn/wanda/lib/data.py", line 43, in get_c4
    traindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')
  File "/users/aca22yn/.local/lib/python3.9/site-packages/datasets/load.py", line 1791, in load_dataset
    builder_instance.download_and_prepare(
  File "/users/aca22yn/.local/lib/python3.9/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/users/aca22yn/.local/lib/python3.9/site-packages/datasets/builder.py", line 1004, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File "/users/aca22yn/.local/lib/python3.9/site-packages/datasets/utils/info_utils.py", line 91, in verify_splits
    raise ExpectedMoreSplits(str(set(expected_splits) - set(recorded_splits)))
datasets.utils.info_utils.ExpectedMoreSplits: {'validation'}
srun: error: gpu13: task 0: Exited with exit code 1
